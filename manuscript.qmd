---
title: "Manuscript"
format: pdf
editor_options: 
  chunk_output_type: console
bibliography: resources/bib.bib
csl: resources/citation-style.csl
link-citations: true
---

# 1. Introduction
Disposition: 

* Recent focus on reproducibility and transparency 
 + Implications for scientific workflow, collaboration, transparency  
* Lack of guidelines and examples of scientific workflows collaborative infrastructure which ensures this
* Aim of this assignment...


\textcolor{red}{The text below is a first draft of the introduciotn. The text itself needs alot of work to convey the scientific aims and background more understanble and precise. However, i feel that my overreaching "red line" and idea for the introducion is covered in this draft, and I want your thoughts in the logic of the text before revisiting it further. In addition, References are kept outside of the document due to possible conflicts when merging branches.}

Computational reproducibility and transparency are core principles of scientific research, essential for ensuring the validity, reliability, and credibility of scientific results. These principles facilitate independent verification, foster critical evaluation, and enable the accumulation of robust scientific knowledge. By adhering to these principles, researchers enhance methodological rigor, promote accountability, and strengthen trust in scientific results. As such, computational reproducibility and transparency are not only key for scientific progress but also represent ethical imperatives in the advancement of scientific understanding. However, researchers often face challenges in achieving this due to the requirement for specialized computational frameworks and the absence of guidelines for scientific workflows that facilitate these practices. 

Achieving computational reproducibility and transparency is further complicated when researchers collaborate on scientific projects due to the complexity and often unstructured nature of scientific workflow. For example, multiple versions of files may coexist, with changes made in parallel, increasing the risk of inconsistencies and loss of critical information. Although a more structured approach to tasks such as project planning, data analysis, and scientific writing could significantly improve collaboration, the lack of standardized workflows and shared frameworks often hinders the implementation of such practices. 

To address the challenges of collaboration in scientific projects, this assignment highlights tools and workflows designed to enhance organizational efficiency and ensure reproducibility. This assignment aims to (1) describe key components of robust, reproducible, and transparent scientific workflows, including setting up a collaborative work space, conduction simulations prior to data acquisition, including data packages for distribution, and creating visualizations from data packages, and (2) present collaborative framework for managing joint research and writing efforts, with a focus on version control systems, R, and GitHub, to streamline the creation of a transparent and reproducible framework for scientific workflows. While the primary outcome of this assignment is a PDF report, the accompanying GitHub repository provides a comprehensive view of the collaborative workflow and serves as an integral part of the project. 



# 2. Setting Up a Collaborative Workspace

-   How to create a repository in Git Hub
-   How to link it to Rstudio
-   How to create a project in your local computer and export it to GitHub
-   Basic commands for Git and GitHub fromt he RStudio terminal
In this tutorial we assume that you will be working with RStudio and have already downloaded R and Rstudio. In addition you are going to need to have the version control software Git installed and have an account in GitHub. If you need guidance for this you can find a helpful tutorial [here](1%20Setting%20up%20your%20software%20environment%20–%20A%20Crash%20Course%20in%20R).

### What is a repository?

A repository is basically like a project box where you collect all the files, data, graphs and code scripts from your project.\
Online repositories can be accessed from the internet and from any computer, while a local repository is only stored in a specific computer and cannot be accessed elsewhere. When setting up a collaborative work space its advantageous to have an online repository so that multiple people can contribute from their own computer to a shared repository, without having to send files by mail etc. In addition we can connect the online repository what a local repository which allows us to work and make changes using our own computer and then we can upload it to the online repository.


### What is Git?
Git is a version control software that allows you to track the different version of your files. It basically allows you to keep a detailed history of changes you have done in your document and also what other people have added or removed in your collaborative documents. Having a version controls software set up for your workflow is very handy as it prevents major losses of documents and changes, and if any error is introduced in a document or code, you can track it back to see what and who submitted it. This fosters reproducibility, transparency, collaboration and robustness for your project.

### What is GitHub?

GitHub is a collaborative online platform that allows you to host and join online repositories. its kinda like facebook for coding. gitHub allows us to share and collaborate with the people on the same code at the same time. It can also be used to host webpages and other stuff.

In this toturial we will only work with the RStudio interface and the online GitHub interface. however, if you want an expanded commandline and interface for GitHub you can use GitHUb CLI and/or GitHub Desktop. See toturials here: [GitHub CLI](GitHub%20CLI%20quickstart%20-%20GitHub%20Docs) and [GitHub Desktop](Getting%20started%20with%20GitHub%20Desktop%20-%20GitHub%20Docs).

## 2.1 How to create a project that is connected between RStudio and GitHub?

When creating a new project and you want to link your local project with an online repository, you can go about it to ways basicly.\
a) You can create the online repository and then clone it down to you computer\
b) You can create a local repository and then push it online to GitHub

We´ll go through both options here, starting with the online repository.

Image file path:

resources/images/

\newpage

### 2.1.1 Starting with an online repository

#### Step 1. Create a new online repository in GitHub ([Video tutorial](Creating%20Your%20First%20GitHub%20Repository%20and%20Pushing%20Code%20-%20YouTube))

```{r}
#| echo: false
#| results: asis
cat("
\\begin{wrapfigure}{lt}{0.5\\textwidth}
  \\centering
  \\includegraphics[width=0.38\\textwidth]{resources/images/image_1.png}
  \\caption{Creating a new online repository.}
  \\vspace{-1.5cm} % Adjust vertical spacing below the figure
\\end{wrapfigure}
")

```

Once you´ve logged into GitHub, navigate to the top right corner of your page and find the + tab. Drop it down to reveal the "New repository" option. Click on it.

This will take you ta the repository creation page.\
Here you give your repository a name, a description of what it will entail and wherever it is public or not.

You also have the options of adding a README file and a .gitignore file upon creation, but it is possible to create these after the repository is made as well.

##### README

A README file is a descriptive file that should explain what the project/repository is about, how it is organized and what the data in it means etc. Any additional information you want people to know when using your repository should go into the README.

```{r}
#| echo: false
#| results: asis
cat("
\\begin{wrapfigure}{r}{0.4\\textwidth}
  \\vspace{-1cm} % Adjust vertical spacing to remove extra white space
  \\centering
  \\includegraphics[width=0.50\\textwidth]{resources/images/image_3.png}
  \\caption{Repository setup-page.}
  \\vspace{-2cm} % Adjust vertical spacing to remove extra white space
\\end{wrapfigure}
")

```

##### .gitignore

The .gitignore file is an information file that tells Git what types of files it should track, or specifically not track. This is useful when you for example don't want to track the generated images or graphs from your code, but just your code.

So, now that you have created your first online repository you want to connect it to your local computer.\
You can do this multiple ways, but in this tutorial we´ll use commands in the terminal to initialize

#### **Step 2: Copy the Repository URL**

1.  Go to your GitHub repository page.

2.  Click the green **Code** button.

3.  Copy the repository URL:

    -   For example: **`https://github.com/yourusername/repositoryname.git`**

#### **Step 3: Open RStudio and Clone the Repository**

1.  Open RStudio.

2.  Go to **File** → **New Project** → **Version Control** → **Git**.

3.  Paste the GitHub repository URL into the "Repository URL" field.

4.  Choose a folder on your local computer where you want to clone the repository.

5.  Click **Create Project**.when doing a commit on a file that has been staged, that version of the file goes into the version history. It is also tracked.

And tada! You have now cloned the online repository to your computer! Great job!

#### Step 4 (Optional): Configure Git in RStudio

If this is your first time using Git with RStudio, you’ll need to configure your Git credentials. Open the RStudio terminal (**Tools** → **Terminal**) or navigate to the **terminal tab** at the top right of the RStudio interface and run the following commands:

``` bash
git config –global user.name "Your Name"

git config --global user.email "your_email@example.com"
```

Replace the placeholder names in "Your Name" and "your_email\@example.com" with your own.

Also, if Git has not yet been initiated in your RStudio project you can use the command:

``` bash
git init
```

To check whet ever Git is initialized in your project you can write:

``` bash
git status
```

If git is not initialized an error message will show up. Then just run the `git init` command and follow the instructions.

### 2.1.2 Starting with a local project in RStudio

Now, what if you wanted to do it the other way around, like if you already have a local project on your computer and want to create an online repository for it?

#### **Step 1. Create a New Local Project in RStudio**

If you already have a local project, you can skip this step. If not:

1.  Open **RStudio**.

2.  Go to **File \> New Project \> New Directory \> New Project**.

3.  Choose a folder where you want the project to live and give it a name.

4.  Make sure to check the box **Create a Git repository**.

5.  Click **Create Project**.

This initializes a local Git repository in your project directory.

If you already have made a project but it is not connected to a Git repository you can do it like this:

1.  Navigate to **Tools \> Project Options \> Git/SVN**.
2.  Select **Git** and click **Yes** when prompted to initialize a Git repository for your project.

#### Step 2. Create a New Repository on GitHub

1.  Create a new repository in Git Hub like previously in section 2.1.1 step 1

    -   Do **not** initialize the repository with a README, **`.gitignore`**, or license (we’ll connect the existing local repository later).

You now have a new, empty GitHub repository.

#### Step 3. Link the Local project to the GitHub Repository

1.  Copy the URL in the same way as previously in section 2.1.1 step 2.
2.  Open the **Terminal** in RStudio (or use any terminal on your computer).
3.  Navigate to your project folder, if you're not already there, by clicking on the dropdown menu in the top right corner of the RStudio interface and choose your project.
4.  Add the GitHub repository as the remote origin using this command in the terminal:

``` bash
git remote add origin https://github.com/yourusername/your-repository.git
```

5.  Verify the remote connection using this command in the terminal `git remote -v`

You should be able to see something like this:

``` bash
origin  https://github.com/yourusername/your-repo.git (fetch)
origin  https://github.com/yourusername/your-repo.git (push)
```

That means you have now successfully established a connection between your local project and the online repository on GitHub. Congratulations!

## 2.2 Workflow between local and online repositories

Now that we have connected our local repository with the online one, we can start to pushing some code! But before we jump into the commands for transferring files between our local and online repository, we should better understand how these processes work.

Take a look at the figure below:

```{r}
#| echo: false
#| warning: false
#| fig-align: "center"
#| out.height: "60%"
#| out.width: "100%"
#| fig-cap: "Visaluzation of local and online workflow in Rstudio."

knitr::include_graphics("resources/images/Local_vs_online_workflow.png")
```

Your **working tree** (also called **working directory**) is the project folder you are currently working in on your local computer. This is where you do all your edits on your files and code.

The **index/staging area** is a list is a list of files that Git is tracking for changes. When you "stage" files you tell Git to include these files in the next commit.

The **local branch** is a version of your project that exists entirely on your local computer. When you "commit" changes, you create a permanent snapshot of the files currently staged in the index and save them to your local repository.

The **remote tracking ref** is where you track the state of the online repository. When you "fetch" changes from the online repository, Git downloads the latest updates from the online repository but without merging it into your working directory. This allows you to see changes from collaborators or updates from the remote repository while keeping your working tree unaffected until you explicitly merge or rebase the changes.

#### 2.2.1 Commands

Okay, now we can take a look at the commands we can use for this workflow.

Lets say you have edited some files in your working directory and want to add them to your **staging area** before committing them to your repository.  If you only want to "stage" some files, but not all, you can use the command in the terminal:

``` bash
git add file_name.txt
```

Just replace the file_name.txt with the name of the file that you want to stage. Remember to also include the file extentions (like .txt or .pdf etc.)

If you want to stage **all files in your repository** that have had changes to them you can write:

``` bash
git add -A
```

This command stages **all changes** in the repository, including modified files, newly created files and deleted files.

Great! Now your edited files are ready to be committed! You can commit your files using this command:

``` bash
git commit -m "Commit message"
```

When you run this commit command, all the files that you have staged are committed to your **local branch!** Its good to include a **descriptive commit message** that explains what your commit entails. That way its easier to track where changes or errors are introduced in your repository. Messages could be "Changed font headings" or "Added statistical ANOVA analysis to data processing".

Now that you have committed the files you want to push them to your remote directory using:

``` bash
git push
```

This command uploads everything you have committed so far this session to your online repository and updates it based on your commits. Using this command you can push multiple commits at the same time.

Now lets say your colleague, who you are collaborate with, has added a new file to the online repository on GitHub. You want to pull that document from the online directory down to your local repositopry and start editing it. The most straightforward way is to use:

``` bash
git pull
```

This command pulls the latest changes from your remote repository and merges them into your **current branch** in your local repository, so that they are exactly the same. Then you can just start editing and making changes.

If you don't want to fully copy the online repository yet, but take a look at it first before merging it ito your working directory you can use the command

``` bash
git fetch
```

This command downloads the changes from the remote repository (e.g., **`origin`**) and updates your remote-tracking branches (e.g., **`origin/main`**) without modifying your working directory or local branch.

To look at the branch you have fetched you can use one of these two commands:

``` bash
git log HEAD..origin/main

git diff HEAD..origin/main
```

**git log** will give you a list of the commits that are not in your local branch (**HEAD**) together with their metadata (authors, date etc.). This is helpful if you want to quickly answer "What was changed?" and "Who changed it?".

**git diff** will show you the actual changes that are between two points, ex. your local branch (**HEAD**) and your fetched branch (**origin/main**). git diff will show you the specific lines of code that were added modified or deleted in the fetched branch. This is helpful if you want to know exactly what was changed.

If you then want to integrate the fetched branch with your current branch, you can use the command:

``` bash
git merge
```

Using this command, Git creates a **merge commit**, which combines the changes from both branches while preserving the complete commit history of both branches. The merge commit explicitly shows the point where the branches diverged and were brought together.

An alternative way to integrate the fetched branch is to use:

``` bash
git rebase
```

This command does **not create a merge commit**. Instead, it **rewrites the commit history** of your current branch by replaying its commits on top of the fetched branch, creating a **linear history**. Essentially, it re-aligns your current branch so that it starts from the latest commit of the fetched branch, as if your changes were made after the fetched branch's changes. This results in a cleaner, more linear commit history.

Questions:

-   add sections about README files and .gitignore

-   should I add a section about the concole and the terminal andhow they are used? Or could this perhaps go into the Introduction?

-   Should we include stuff about creating branches?

**Disclaimer:** This section was written with the help of SIKT KI chat using the gpt-4o model. The AI was used to verify code chunks, summarize steps in an organized format and rewrite original text for better grammar and flow. The author takes full responsibility for the resulting output. [Link to AI model](Sikt%20KI).

\newpage

# 3. Conducting Simulations Before Data Acquisition
```{r}
#| echo: false
#| warning: false
#| results: false
#| label: Required packages


library(tidyverse)
library(ggdag)
```


Conducting simulations prior to data acquisition is useful for several reasons, including: 

1. *Model design*. By sampling from the prior distributions, we can understand our 
expectations about potential outcomes. This pre-data exploration offers insights 
into the implications of the prior assumptions.
2. *Model checking*. Once a model is updated using real data, simulating implied 
data can help assess the success of the fit and explore the model's behavior.
3. *Software validation*. In order to ensure that our model fitting software is
functioning correctly, it is helpful to simulate observations under a known model 
and then attempt to recover the parameter values from which the data were simulated. 
4. *Research design*. Simulating observations based on our hypothesis allows for 
an evaluation of the research design's effectiveness. This is similar to conducting 
a *power analysis*, but the possibilities are broader.   
5. *Forecasting*. Estimates derived from simulations can be utilized to generate 
predictions for new cases and future observations [@McElreath_2018].

To illustrate some of the properties of conducting simulations, we will consider 
the following hypothesis: **Increasing daily exercise time is associated with lower 
blood pressure levels in adults**. 


To investigate this hypothesis, it is valuable to invest effort into finding methods 
that distinguish causal inferences from associations. A helpful first step to do 
this is to construct a causal model that is separate from the statistical model. 
The simplest graphical representation of such a causal model is a *DIRECTED ACYCLIC GRAPH*, 
commonly referred to as a *DAG* [@McElreath_2018; @digitale_tutorial_2022].

A DAG primarily serves to represent our prior knowledge about biological and behavioral systems that may confound the specific causal research question. In our example, we aim to investigate the causal effect of increasing exercise time, denoted as "E", on blood pressure, "B". A basic DAG for this relationship might look like this (Figure 1):


```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "A DAG, assuming that the causal effect of increasing exercise time (E) on blood pressure (B) is not confounded."
#| label: Fig-dag1
#| fig-width: 6
#| fig-height: 3



coord <- tibble(name = c("E", "B"),
                x = c(1, 2),
                y = c(1, 1))

dag <- dagify(B ~ E,
              coords = coord)


fig1 <- dag |> 
  tidy_dagitty() |>
  ggplot(aes(x = x, y = y,
             xend = xend, yend = yend)) +
  geom_dag_point(color = "white") +
  geom_dag_text(color = "black", size = 8) +
  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(5, "pt"), 
                                              type = "closed")) +
  
  theme_dag()



fig1

```


This DAG assumes that the causal effect of increasing exercise time on blood pressure 
is not confounded, which is arguably a naive assumption. For instance, both age and 
diet could potentially confound this relationship. A more comprehensive DAG that 
accounts for these confounders might appear as follows (Figure 2):


```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "A DAG, assuming that the causal effect of increasing exercise time (E) on blood pressure (B) is confounded by both age (A) and diet (D)."
#| label: Fig-dag2
#| fig-width: 6
#| fig-height: 3



coord <- tibble(name = c("E", "B", "A", "D"),
                x = c(1, 2, 1.2, 1.7),
                y = c(1, 1, 2, 2))

dag <- dagify(B ~ E,
              B ~ A,
              E ~ A,
              B ~ D,
              coords = coord)


fig1 <- dag |> 
  tidy_dagitty() |>
  ggplot(aes(x = x, y = y,
             xend = xend, yend = yend)) +
  geom_dag_point(color = "white") +
  geom_dag_text(color = "black", size = 8) +
  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(5, "pt"), 
                                              type = "closed")) +
  
  theme_dag()



fig1

```


In this adjusted DAG, we presume that age directly influences blood pressure— an 
assumption supported by the correlation between aging and increased blood pressures 
[@Miall_1967]. Additionally, age may indirectly influence exercise time since older 
individuals often engage in less physical activity. Diet is also included as a confounding 
factor affecting blood pressure, linked to dietary habits like high-fat intake [@Wilde_2000].

Given that we believe our DAG to be accurate, we can construct our model with stronger 
scientific justifications regarding which confounding factors should be accounted 
for and how we will do so.

**Model design** 

Having established the confounding factors of age and diet in the relationship between 
exercise time and blood pressure, we are now prepared to design our study. We plan 
to recruit 100 sedentary adults aged between 20 and 70 for a one-year training study, 
during which participants will adhere to the WHO guideline of 150 minutes of endurance 
exercise per week. Unfortunately, we will only be able to measure blood pressure 
after the intervention period due to logistical constraints. Therefore, we can only 
investigate whether there is an association between how many minutes of physical 
activity the participants complete and their blood pressure levels after the intervention. 
To control for age and diet, we will document participants' ages at the time of inclusion 
and assess their diets using a binomial scale, where 1 indicates a healthy diet and
0 indicates an unhealthy diet. 

The model can now be described mathematically, for example in terms of a simple 
linear regression model:


\begin{align}
\operatorname{BloodPressure}_i &= \beta_0 + \beta_1 \cdot \operatorname{Exercise}_i 
+ \beta_2 \cdot \operatorname{Age}_i + \beta_3 \cdot \operatorname{Diet}_i + \epsilon_i \\
\end{align}




Where: 

\begin{align}
\beta_0 &: \text{Intercept}\\
\beta_1 &: \text{Coefficient for exercise time}\\
\beta_2 &: \text{Coefficient for age}\\
\beta_3 &: \text{Coefficient for diet}\\
\epsilon_i &: \text{Random error term}\\
\end{align}





The linear regression model can be implemented in R as follows: 

```{r}
#| results: false
#| label: Example data and model

# Example dataset
data <- data.frame(
  exercise = rnorm(100, mean=150, sd=20),
  age = rnorm(100, mean=45, sd=15),
  diet = rnorm(100, mean=2, sd=1),
  bloodpressure = rnorm(100, mean=120, sd=10)
)

# Fit a simple linear regression model
model <- lm(bloodpressure ~ exercise + age + diet, data = data)

# Summarize the model results
summary(model)
```

**Run simulations**

We will now run simulations to evaluate our model design. First, we need to define 
our parameter values:


\begin{align}
\operatorname{BP}_i & = \beta_0 + \beta_E \cdot \operatorname{E}_i + \beta_A \cdot \operatorname{A}_i + \beta_D \cdot \operatorname{D}_i + \epsilon_i \\
\operatorname{E}_i & \sim \operatorname{Normal}(\mu_E, \sigma_E) \\
\operatorname{A}_i & \sim \operatorname{Normal}(\mu_A, \sigma_A) \\
\operatorname{D}_i & \sim \operatorname{Binomial}(n=1, p=0.5) \\
\epsilon_i & \sim \operatorname{Normal}(\mu_\epsilon, \sigma_\epsilon) \\
\beta_0 & = 120 \quad (\text{intercept}) \\
\beta_E & = -0.1 \quad (\text{beta for exercise}) \\
\beta_A & = 0.2 \quad (\text{beta for age}) \\
\beta_D & = -5 \quad (\text{beta for diet}) \\
\mu_E & = 150 \quad (\text{mean exercise}) \\
\mu_A & = 45 \quad (\text{mean age}) \\
\mu_\epsilon & = 0 \quad (\text{mean error term}) \\
\sigma_E & = 20 \quad (\text{exercise standard deviation}) \\
\sigma_A & = 10 \quad (\text{age standard deviation}) \\
\sigma_\epsilon & = 5 \quad (\text{error term standard deviation}) \\
\end{align}



After defining our parameter values, we can simulate data using these parameters. 
To do this, it is convenient to create a function where you specify information about 
each parameter that can be manipulated later, thereby enabling experimentation with 
different values to assess the model's robustness. Below is the code for this simulation:

```{r}
#| results: false
#| label: Sim.function


# Simulate data based on the hypothetical model

# A basic function
sim.fun <- function(n = 100,
                intercept = 120,
                mean_exercise = 150,
                beta_exercise = -0.1,
                sigma_exercise = 20,
                mean_age = 45,
                beta_age = 0.2,
                sigma_age = 10,
                size_diet = 1,
                prob_diet = 0.5,
                beta_diet = -5, 
                mean_epsilon = 0,
                sigma_epsilon = 5) {
  
  # This is the body 
  epsilon <- rnorm(n, mean = mean_epsilon, sd = sigma_epsilon) #error term 
  diet <- rbinom(n, size = size_diet, prob = prob_diet) #simulate diet 
  age <- rnorm(n, mean = mean_age, sd = sigma_age) #simulate age
  exercise <- rnorm(n, mean = mean_exercise, sd = sigma_exercise) #simulate exercise duration
  bloodpressure <- intercept + (beta_exercise * exercise) + (beta_age * age) + 
    (beta_diet * diet) + epsilon
  
  
  
  # The output
  return(data.frame(age, exercise, diet, bloodpressure, epsilon))
  
}


# Store the data frame as "sim_data"
sim_data <- sim.fun()


```

With our simulated dataset, we can now test our hypotheses using the linear model: 
*lm(bloodpressure ~ exercise + age + diet, sim_data)*. The output will indicate whether
there is an observed effect of exercise on blood pressure:

```{r}
#| echo: false
#| label: Simulatetd.m1

set.seed(1)
sim_data <- sim.fun()


sim.m1 <- lm(bloodpressure ~ exercise + age + diet, sim_data)

summary.m1 <- summary(sim.m1)

```



```{r}
#| echo: false
#| label: Sim.m1.output 


summary.m1

exercise_effect <- sprintf("%.3f",summary(sim.m1)$coefficients["exercise", "Estimate"])

```

In this model, we can see that each minute of increased exercise time is associated 
with a **`r exercise_effect`** mmHg lower blood pressure, when age and diet 
are included as covariates.

Now it is time to play with our simulation! Lets say we only manage to include 50 
participents, what will happen whit our results?

This can easily be tested by manipulating our function and then running the model, 
like this:


```{r}
#| label: Manipulate function 

# Changing number of observations to 50!
set.seed(7)
sim_data2 <- sim.fun(n = 50)


```



```{r}
#| echo: false
#| label: Sim.m2.output 


sim.m2 <- lm(bloodpressure ~ exercise + age + diet, sim_data2)

summary.sim.m2 <- summary(sim.m2)

summary.sim.m2

exercise_effect2 <- sprintf("%.3f", summary(sim.m2)$coefficients["exercise", "Estimate"])
```


The output for this model may reveal that the **`r exercise_effect2`** mmHg effect 
of exercise is no longer significant, suggesting that 50 participants may not be 
sufficient to detect the true effect of exercise which we know is **-0.1** mmHg. 
However, this is just one simulation; results may vary due to random chance. It is 
therefore prudent to conduct multiple simulations to evaluate the robustness of our 
model, which can be executed in a loop as demonstrated below:




```{r}
#| label: Simulations loop

set.seed(1)
results <- list()

for(i in 1:1000) {
  
  dat1 <- sim.fun()
  dat2 <- sim.fun(n = 50)
  
  
  m1 <- lm(bloodpressure ~ exercise + age + diet, data = dat1)
  m2 <- lm(bloodpressure ~ exercise + age + diet, data = dat2)

  
  results[[i]] <- data.frame(model = c("m1", "m2"),
             estimate = c(coef(m1)[2],
                          coef(m2)[2]))
  
  
  
}


```



To visualize the estimates of model 1 and model 2, it is convenient to create a 
figure similar to Figure 3.


```{r}
#| label: Simulation figure 
#| echo: false
#| fig-cap: "Demonstrating the variation in the estimated effect of exercise on blood pressure using 100 participants (m1) compared to 50 participants (m2) by simulating 1,000 different datasets."


bind_rows(results) |>
  ggplot(aes(model, estimate)) +
  geom_point(position = position_jitter(width = 0.1),
             alpha = 0.2, 
             shape = 21,
             color = "blue") + 
  theme_classic()
```


In the simulated models shown in Figure 3, the estimate for model m1 clusters more 
tightly around -0.1, which represents the true effect of exercise on blood pressure
as specified. In contrast, model m2 exhibits more variation, indicating that the 
effect is less reliably detected with only 50 participants.

This brief introduction illustrates how to conduct basic data simulations before 
data acquisition, empowering you to enhance the robustness of your research! 

# 4. Including Data Packages for Distribution

# 5. Creating Visualizations from Data Packages

The aim of this section is to demonstrate a workflow from data package to visualization of your data, with a focus on reproducibility and collaboration. The reproducibility is already established through the infrastructure provided by having a data package available - no spread sheets or CSV files floating around untracked in different states between collaborators. With a data package available the data remains stable, tracked, accessible and identical to all collaborators. This means that your scripts/qmds for data visualization can focus solely on that purpose, i.e., joining, wrangling and plotting data. On that note, clear and descriptive comments about how you wrangle and plot your data are essential - both for your future self and for collaborators. This practice makes it much easier to resume work and understand the reasoning behind decisions made in the visualization of your data.

### Prior to plotting

Since we already have a data package that cleans and prepares our data for visualization, we can simply import the data from this package. The package is organized by test, meaning one xlsx file per test containing only the measurements from that specific test. Therefore, additional wrangling is necessary to incorporate other information we want to visualize, such as age group, condition, and participant allocation. This data preparation process can be surprisingly time-consuming, but investing effort in creating tidy data makes the actual plotting (and your life) considerably easier.

```{r, data import and wrangling}
library(tidyverse)
#library(dplyr)
library(writexl)

## Importing the data from the data package

# Importing measurements from the humac dynamometer test
humac.dat <- Final.reportData::humac.dat # this is the peak torque test data

# Importing information about each participants volume condition
condition <- Final.reportData::condition %>%
  mutate(participant = as.character(participant))

# Importing infromation about the participants age groups and allocation
participants <- Final.reportData::participants %>%
  mutate(age_group = if_else(age < 40, "yng", "old"))



## Joining the data sets
# Using left_join to add the info from condition and participants to humac.dat, we'll also remove the info about arm condition, since our data does not contain measurements of arm muscle peak torque. 
humac.dat <- humac.dat %>%
  left_join(condition) %>%
  select(-arm) %>% 
  left_join(participants) %>%
  mutate(age_group = factor(age_group, levels = c("yng", "old")),
# using factor() to sort the order of the respective variables, meaning they will always be displayed in this order in analyses and plots - unless manually overridden. I think this is a nice way to ensure consistency across multiple plots.
         
         condition = factor(condition, levels = c("low", "mod")),
         
         time = factor(time, levels = c("pre", "mid",  "post")),
         
         speed = factor(speed, levels = c("0", "60", "120", "240")),
         
         allocation = factor(allocation, levels = c("int", "con")),
# case_when makes sure that control participants are not grouped with a volume condition
         condition = case_when(participant %in% c("3071", "3075", "3076", 
                                                  "3083", "3084", "3085",
                                                  "3086", "3090", "3091",
                                                  "3094") ~ "none", TRUE ~ condition))

# Now we have a complete data set with the measurements from the humac test, as well as information about what age group, allocation and condition all the participants had. At this point (or even before) you might want to use the glimpse() function to have a look at the data set and all the variables it contains - here you can also check that what we've joined in is actually there!

glimpse(humac.dat)

## Further data wrangling
# The data set contains 2x test per time point, so we could either average the two measurements per time point or select the highest value from each. We'll do the latter here. This can be done using the summarise() function alone, however, as we saw with glimpse(), there are multiple "improper" NA's and corresponding errors. Therefore I've included the below code, which makes a functions that returns proper NA values where all values are NA for a group.


safe_max <- function(x) {
  if (all(is.na(x))) return(NA_real_)
  max(x, na.rm = TRUE)
}

safe_min <- function(x) {
  if (all(is.na(x))) return(NA_real_)
  min(x, na.rm = TRUE)
}


## Step-by-step explanation:
#1. function(x) - This creates a function that takes one input called x (which will be your vector of values)
#2. if (all(is.na(x))) - This checks the condition:

#is.na(x) checks each element and returns TRUE/FALSE for each
#all() checks if ALL elements are TRUE
#So this asks: "Are ALL values in x missing (NA)?"

#3. return(NA_real_) - If all values are NA:

#Return NA_real_ (which is specifically a numeric NA, as opposed to NA_character_ or NA_integer_)
#return() immediately exits the function

#4. max(x, na.rm = TRUE) - If NOT all values are NA:

#Calculate the maximum, ignoring any NAs that exist
#This line only runs if the if condition is FALSE

## Get max values for all outcomes
# With the new "safe_max" variable, we can safely get the highest measurements without producing erroneous NAs.

max.dat <- humac.dat %>%
  filter(!participant == "3012") %>% # Missing data
  group_by(participant, time, leg, speed, condition, age_group, allocation, sex) %>%
  summarise(pt_max = safe_max(pt),
            power_max = safe_max(rep_power),
            work_max = safe_max(rep_work),
            tt_min = safe_min(pt_tt),
            angle = mean(pt_angle, na.rn = TRUE),
            .groups = "drop")


## Reshape the data to long format
# This simply formats the data to where each observation is stored in its own row, with variables spread across columns - thus minimizing redundancy. This typically makes the data frame more tidy, and is quite practical for selecting variables, faceting etc, and will remain the go-to data frame for all the plots here. 

long.dat <- max.dat %>%
  pivot_longer(names_to = "outcome",
               values_to = "value",
               cols = c(pt_max, power_max, work_max, tt_min, angle))


```

### Plotting some figures

Now that the data is imported and tidied up, lets start with a simple plot to get an overview. The backbone of all these plots is the ggplot() function from ggplot2 (loaded by tidyverse), supplemented by cowplot for additional functionality. Briefly, ggplot2 implements a layered grammar of graphics, providing tools to build plots by mapping variables to aesthetics, geometric objects, and facets, then adding themes and annotations [@wickham_2010].

Before attempting to create a polished final figure, explore your data with simple plots—this is valuable both for visualization and before statistical analysis. Since we've already wrangled the data into tidy long format and examined the variables of interest with glimpse(), we can proceed directly to plotting.

**Plot 1: Simple Visualization**

```{r, Plot1}

## Simple overview plot

overview.plot <- long.dat %>%
  ggplot(aes(speed, value, group = age_group, color = age_group)) +
  geom_point() + # maps data to points in the plot
  theme_minimal()

# So, this simple plot does not really do much other than confirming that there is data and its being mapped to the aesthetics we call. Since this data has multiple levels of grouping, it would be a good idea to add some facets via the facet_wrap() function. Further, this plot generated a warning message that 876 rows either contained missing values or values outside the scale range was being removed - this occurs due because we didn't specify what outcome we wanted to look at, thus including all outcomes with a wide difference in scale. 

## Faceting by angular velocity  
# Here we'll use filter() to grab exactly the outcome measure we want and nothing else to fix the scale range issue, and we'll use facet_wrap to divide the plot into lesser plots for each angular velocity.

facet.plot <- long.dat %>%
  filter(outcome == "pt_max") %>% 
  ggplot(aes(time, value, group = age_group, color = age_group)) +
  geom_point() +
  facet_wrap(~speed) + 
  theme_minimal()

facet.plot

```

This is a reasonable start—we have angular velocity separated by facets and data points colored by age group. However, to visualize the change from baseline to post-intervention, additional considerations are needed. In general, the process onward will depend on your research question and design [@tufte_2001], but with the present data, we may expect differences based on age group (young vs. old), training volume (low vs. moderate), and participant allocation (intervention vs. control), so the plot must account for these factors. This can be accomplished using cowplot alongside faceting to display all relevant groups without overcrowding a single graph.

**Plot 2: Individual Changes in Peak Torque at 0 (isometric), 60, 120 and 240 d/S**

In this example plot, we create one plot per angular velocity, and facet by age group and allocation. This will give us a good overview of what happens in each of the groups from baseline to the half-way mark, and then to post.

```{r, Plot2}

## Peak torque Isometric

plot.isom <- long.dat %>%
  # First we specify what angular velocity and outcome data we want with filter()
  filter(speed == "0",
         outcome == "pt_max") %>% 
  
  ggplot(aes(x = time, y = value, group = interaction(participant, leg))) +    # the interaction(participant/leg) creates a unique grouping variable combining participant and leg - so we can account for the fact that the participants exercised with a unilateral protocol
  
  geom_line(aes(color = condition), alpha = 0.3) +
  geom_point(aes(color = condition), alpha = 0.3, size = 1) +                  # color = condition makes sure the points and lines are colored by condition
  
  facet_wrap(vars(age_group, allocation), scales = "fixed") +                  # vars() can be used like this when you would like to facet_wrap on multiple groupings. Scales are set to "fixed" to have the same scale for all the plots in our final cowplot
  
  scale_color_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                     name = "Training Volume") + # this sets the color of geom_line and geom_point, ideally to something easily distinguishable 
  
  labs(title = "",
       subtitle = "",
       x = "",
       y = "Peak Torque (Nm)") + # this can be changed to any suitable title, subtitle etc. when using cowplot to grid multiple plots, you might want to remove these ("") for most of or all your plots, since a common title etc. can be made in the cowplot. 
  
  theme_minimal() +
  theme(legend.position = "bottom",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "black"),
        axis.line.x = element_blank(),
        axis.text.x = element_blank())#,
       # strip.text = element_blank()) # these theme() changes are part of the final cosmetic adjustment of each plot. For instance strip.text = element_blank() removes facet labels, and this could be useful to apply in some of the plots as long as they follow the same pattern. 

# The following three plots will be copies of the above one, with slight differences in theme/labs to fit together in the cowplot. 

## Peak torque 60

plot.60 <- long.dat %>%
  filter(speed == "60",
         outcome == "pt_max") %>%
  
  ggplot(aes(x = time, y = value, group = interaction(participant, leg))) +
  geom_line(aes(color = condition), alpha = 0.3) +
  geom_point(aes(color = condition), alpha = 0.3, size = 1) +
  facet_wrap(vars(age_group, allocation), scales = "fixed") +
  scale_color_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                     name = "Training Volume") +
  labs(title = "",
       subtitle = "",
       x = "",
       y = "") +
  theme_minimal() +
  theme(legend.position = "bottom",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "black"),
        axis.line.x = element_blank(),
        axis.text.x = element_blank())#,
        #strip.text = element_blank())

## Peak torque 120

plot.120 <- long.dat %>%
  filter(speed == "120",
         outcome == "pt_max") %>%
  ggplot(aes(x = time, y = value, group = interaction(participant, leg))) +
  geom_line(aes(color = condition), alpha = 0.3) +
  geom_point(aes(color = condition), alpha = 0.3, size = 1) +
  facet_wrap(vars(age_group, allocation), scales = "fixed") +
  scale_color_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                     name = "Training Volume") +
  labs(title = "",
       subtitle = "",
       x = "Time",
       y = "Peak Torque (Nm)") +
  theme_minimal() +
  theme(legend.position = "bottom",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "black"),
        axis.text.x = element_blank(),
        strip.text = element_blank())

## Peak torque 240

plot.240 <- long.dat %>%
  filter(speed == "240",
         outcome == "pt_max") %>%
  ggplot(aes(x = time, y = value, group = interaction(participant, leg))) +
  geom_line(aes(color = condition), alpha = 0.3) +
  geom_point(aes(color = condition), alpha = 0.3, size = 1) +
  facet_wrap(vars(age_group, allocation), scales = "fixed") +
  scale_color_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                     name = "Training Volume") +
  labs(title = "",
       subtitle = "", #Each line represents one leg from one participant
       x = "Time",
       y = "") +
  theme_minimal() +
  theme(legend.position = "bottom",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "black"),
        axis.text.x = element_blank(),
        strip.text = element_blank())


library(cowplot)

## Combining the plots
# This is where we can use cowplot to combine all the four plots above into one plot - and yes, you can combine different cowplots into an even bigger plot if you wanted to. 

# This one is quite basic, but suits our current needs quite well. The plots will appear in the order they are called, and you can define how many columns and rows you want with ncol and nrow. Since we use theme() to hide the legend of two of the plots here, we can adjust the relative hight of the upper row vs. the lower with rel_heights. 

ptfig <- plot_grid(
  plot.isom + theme(legend.position = "none"),
  plot.60 + theme(legend.position = "none"),
  plot.120,
  plot.240,
  ncol = 2,
  nrow = 2,
  labels = c("A) 0 d/s", "B) 60 d/s", "C) 120 d/s", "D) 240 d/s"),
  label_size = 10,
  rel_heights = c(0.8, 1) 
)


ptfig



```

**Plot 3: Mean Change Peak Torque per Age Group - with raw data**

While it may be fascinating to look at all the individual lines in these plots, it's not necessarily easy to discern what happens in the respective groups here. So, another option is to summarize the change in peak torque instead of having all the individual lines. In addition we can use either errorbars or map the individual data to display the variation in the data frame - technically you could do both, but that would be quite messy in our case.

```{r, Plot3}

# Set seed for jitter

set.seed(1) # further down we will use position_jitter to spread out our data points a bit more, set.seed(1) simply makes this spread reproducible instead of randomly changing every time you run the code. Notably the (1) doesnt really matter, it could be (22), as long as its the same every time for reproducibility. 

# The setup here will be quite similar to the previous plot, but we do need to perform some statistical transformation using summarise() to get the mean change instead of all the individual changes. 

# Isometric 

# First we create a data frame containing only the isometric angular velocity and the outcome peak torque. The reason for this, is that we would like to use this specific data frame for our geom_points further down, after we have summarised the data. Thus we need a separate data frame. 
isom.dat <- long.dat %>%
  filter(speed == "0",
         outcome == "pt_max")

  
plot.isommean <- isom.dat %>%
  
#summarising means - before summarising we select what we would like to group our data by with group_by(), and we then use .groups = "drop" when the summarizing is completed.
  group_by(age_group, allocation, condition, time) %>%
  summarise(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            .groups = "drop" ) %>%
  
  #creating the plot
  ggplot(aes(x = time, y = mean, color = condition, group = condition)) +
  
  # Here we can use two instances of geom_point to map out both the mean data and the indivdual data. For the mean data, we also add geom_line to connect the dots depending on condition, and we make sure they dont overlap too much by using position_dodge with identical coordinates. 
  # Mean data
  geom_line(linewidth = 1, position = position_dodge(.5)) +
  geom_point(shape = 21, aes(fill = condition), color = "black",
             size = 3, stroke = 1, position = position_dodge(.5)) +
  # Individual data
  geom_point(data = isom.dat, # Here we add the isom.dat with all the individual values intact
             aes(x = time, y = value,
                 color = condition, group = condition),
                 size = 1.5, # adjusts the size of these data points
                 alpha = 0.3, # adjusts the alpha, might be a good idea to have a lower alpha and size compared to the mean data points
             position = position_jitter(width = 0.2)) +
  
# Here's an example of how to implement errorbars instead, if thats your jam
#  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd),
#                width = 0.1, linewidth = 0.8, position = position_dodge(.2)) +

  
  
  facet_wrap(vars(age_group, allocation), scales = "fixed") +
  
  scale_color_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                     name = "Training Volume") +  # <-- hide the color legend
  
  scale_fill_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                    guide = "none") +  # <-- add fill scale
  
  labs(title = "",
       subtitle = "",
       x = "",
       y = "Peak Torque (Nm)") +
  
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "black"),
        axis.text.x = element_blank(),
        axis.line.x = element_blank())


# The following three plots will be copies of the above one, with slight differences in theme/labs to fit together in the cowplot. 

# 60 d/s

dat.60 <- long.dat %>%
  filter(speed == "60",
         outcome == "pt_max")

plot.60mean <- dat.60 %>%
  #summarising means
  group_by(age_group, allocation, condition, time) %>%
  summarise(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            .groups = "drop" ) %>%
  
  #creating the plot
  ggplot(aes(x = time, y = mean, color = condition, group = condition)) +
  
  geom_line(linewidth = 1, position = position_dodge(.5)) +
  geom_point(shape = 21, aes(fill = condition), color = "black",
             size = 3, stroke = 1, position = position_dodge(.5)) +
  geom_point(data = dat.60,
             aes(x = time, y = value,
                 color = condition, group = condition),
                 size = 1.5,
                 alpha = 0.3,
             position = position_jitter(width = 0.2)) +
 # geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd),
#                width = 0.1, linewidth = 0.8, position = position_dodge(.2)) +
  
  
  
  facet_wrap(vars(age_group, allocation), scales = "fixed") +
  
  scale_color_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                     name = "Training Volume") +  # <-- hide the color legend
  
  scale_fill_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                    guide = "none") +  # <-- add fill scale
  
  labs(title = "",
       subtitle = "",
       x = "",
       y = "") +
  
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "black"),
        axis.text.x = element_blank(),
        axis.line.x = element_blank())


# 120 d/s

dat.120 <- long.dat %>%
  filter(speed == "120",
         outcome == "pt_max")

plot.120mean <- dat.120 %>%
  #summarising means
  group_by(age_group, allocation, condition, time) %>%
  summarise(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            .groups = "drop" ) %>%
  
  #creating the plot
  ggplot(aes(x = time, y = mean, color = condition, group = condition)) +
  
  geom_line(linewidth = 1, position = position_dodge(.5)) +
  geom_point(shape = 21, aes(fill = condition), color = "black",
             size = 3, stroke = 1, position = position_dodge(.5)) +
  geom_point(data = dat.120,
             aes(x = time, y = value,
                 color = condition, group = condition),
                 size = 1.5,
                 alpha = 0.3,
             position = position_jitter(width = 0.2)) +
  
 # geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd),
  #              width = 0.1, linewidth = 0.8, position = position_dodge(.2)) +
  
  # adding in the raw data
  
  
  facet_wrap(vars(age_group, allocation), scales = "fixed") +
  
  scale_color_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                     name = "Training Volume") +  # <-- hide the color legend
  
  scale_fill_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                    guide = "none") +  # <-- add fill scale
  
  labs(title = "",
       subtitle = "",
       x = "Time",
       y = "Peak Torque (Nm)") +
  
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "black"),
        strip.text = element_blank())


# 240 d/s

dat.240 <- long.dat %>%
  filter(speed == "240",
         outcome == "pt_max")

plot.240mean <- dat.240 %>%
  #summarising means
  group_by(age_group, allocation, condition, time) %>%
  summarise(mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            .groups = "drop" ) %>%
  
  #creating the plot
  ggplot(aes(x = time, y = mean, color = condition, group = condition)) +
  
  geom_line(linewidth = 1, position = position_dodge(.5)) +
  geom_point(shape = 21, aes(fill = condition), color = "black",
             size = 3, stroke = 1, position = position_dodge(.5)) +
  geom_point(data = dat.240,
             aes(x = time, y = value,
                 color = condition, group = condition),
                 size = 1.5,
                 alpha = 0.3,
             position = position_jitter(width = 0.2)) +
  
  #geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd),
   #             width = 0.1, linewidth = 0.8, position = position_dodge(.2)) +
  
  # adding in the raw data
  
  
  facet_wrap(vars(age_group, allocation), scales = "fixed") +
  
  scale_color_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                     name = "Training Volume") +  # <-- hide the color legend
  
  scale_fill_manual(values = c("low" = "#E69F00", "mod" = "#56B4E9", "none" = "gray50"),
                    guide = "none") +  # <-- add fill scale
  
  labs(title = "",
       subtitle = "",
       x = "Time",
       y = "") +
  
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "black"),
        strip.text = element_blank())


# Combinding the plots

ptimeanfig <- plot_grid(
  plot.isommean + theme(legend.position = "none"),
  plot.60mean + theme(legend.position = "none"),
  plot.120mean,
  plot.240mean,
  ncol = 2,
  nrow = 2,
  labels = c("A) Isometric", "B) 60 d/s", "C) 120 d/s", "D) 240 d/s"),
  label_size = 10,
  rel_heights = c(0.8, 1)
)



ptimeanfig

```

**Plot 4: Mean Peak Torque change in a Hill curve format**

If we were interested in comparing the age groups and intervention vs. control, we could combine all of that into one plot. A cool and illustrative plot with this type of data would be to make a force-velocity curve, first developed by A. V. Hill in 1938 [@hill_1938], and compare the age groups across angular velocities. We do assume that force development is reduced when angular velocity is increased, so it lets visualize it!

```{r, Plot4}

# Calculating group means
curve.sum <- long.dat %>%
  filter(outcome == "pt_max") %>% # this time we're including all angular velocities, so we only filter the outcome
  
  group_by(age_group, allocation, time, speed) %>%
  
  summarise(mean = mean(value, na.rm = TRUE),
            #sd = sd(value, na.rm = TRUE),
            .groups = "drop") %>%
  
  # This time we combine age_group and allocation, since volume condition is not on the menu. 
  mutate(group = paste(age_group, allocation, sep = "_"))
  


# Plot 2: Single plot showing everything (might be busy)
curve.plot <- curve.sum %>%
  
  ggplot(aes(x = speed, y = mean, color = group, linetype = time, 
             group = interaction(group, time))) +
  
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  
  labs(x = "Angular Velocity (d/s)",
       y = "Peak Torque (Nm)",
       color = "Group",
       linetype = "Time Point",
       title = "") +
  
  scale_color_manual(values = c("yng_int" = "#E69F00",
                                  "old_int" = "#56B4E9",
                                  "old_con" = "gray50"),
                     labels = c("yng_int" = "Young Intervention", 
                                "old_int" = "Old Intervention", 
                                "old_con" = "Old Control")) +
  
  theme_classic() +
  theme(legend.position = "bottom")


curve.plot




```

The plots presented here are only examples, and you'll likely iterate through several versions to explore different perspectives of your data, as advised by Tufte [@tufte_2001]. Effective visualization should always aim to convey the data clearly to readers unfamiliar with it. A general workflow—importing data → tidying → creating exploratory plots → refining and tweaking—provides a solid strategy. Additionally, each plot can be thought of as having its own workflow: starting with your data and systematically building layers of aesthetics, geometric objects, facets, and coordinates, before adding annotations (if applicable) and theme adjustments to finalize the presentation [@wickham_2010].



# Bibliography


